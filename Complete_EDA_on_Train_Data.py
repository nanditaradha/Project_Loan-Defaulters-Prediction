# -*- coding: utf-8 -*-
"""Copy of Complete EDA on train_data.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1-5i3Cmaqj5M5zIlsY60wUaLmACQDgzUq
"""

#EDA
#I.Data Sourcing
#Loading Data into dataframe
import pandas as pd
from google.colab import files

from google.colab import drive
drive.mount('/content/drive')

import io

path3 = "/content/drive/My Drive/loan-pred folder/Loan_Data.train.csv"

#Load the Train Dataset
bankdata15= pd.read_csv(path3)
bankdata15.shape   #o/p:(104999, 27)

#Train data
#bankdata15.shape #o/p:(104999, 27)
trndt1 = bankdata15.copy()
#trndt1.shape  #(104999, 27)
trndt1.head(1)

#II.Data pre-processing for Train Dataset
trndt1.shape                                          #o/p:(104999, 27)
trndt1.nunique()
trndt1.info()                                         #o/p:dtypes: int64(11), object(16)
#Found no duplicates in the dataframe
any(trndt1.duplicated())                              #o/p:False
#Number of missing/null values in the data
trndt1.isnull().sum()
#Observed Null values in the following columns with their count:
#o/p:Name=6,City=1,State=1,Bank=111,BankState=112,RevLineCr=14,ChgOffDate=76722,DisbursementDate=156,MIS_Status=615

trndt1.nunique()

trndt1.describe(include='all')

trndt1.nunique()

#finding the level counts of each column in the Train Dataset
#trndt1['Name'].value_counts().count()                #o/p:99629
#trndt1['City'].value_counts().count()                #o/p:13535
#trndt1['State'].value_counts().count()               #o/p:51
#trndt1['Zip'].value_counts().count()                 #o/p:17324
#trndt1['Bank'].value_counts().count()                #o/p:2610
#trndt1['BankState'].value_counts().count()           #o/p:52
#trndt1['CCSC'].value_counts().count()                #o/p:1150
#trndt1['ApprovalDate'].value_counts().count()        #o/p:2339
#trndt1['ApprovalFY'].value_counts().count()          #o/p:20
#trndt1['Term'].value_counts().count()                #o/p:35
#trndt1['NoEmp'].value_counts().count()               #o/p:234
#trndt1['NewExist'].value_counts().count()            #o/p:3
#trndt1['CreateJob'].value_counts().count()           #o/p:88
#trndt1['RetainedJob'].value_counts().count()         #o/p:151
#trndt1['FranchiseCode'].value_counts().count()       #o/p:876
#trndt1['UrbanRural'].value_counts().count()          #o/p:3
#trndt1['RevLineCr'].value_counts().count()           #o/p:7
#trndt1['LowDoc'].value_counts().count()              #o/p:4
#trndt1['ChgOffDate'].value_counts().count()
#trndt1['DisbursementDate'].value_counts().count()    #o/p:1694
#trndt1['DisbursementGross'].value_counts().count()   #o/p:28378
#trndt1['BalanceGross'].value_counts().count()        #o/p:2
#trndt1['MIS_Status'].value_counts().count()          #o/p:2
#trndt1['ChgOffPrinGr'].value_counts().count()        #o/p:20830
#trndt1['GrAppv'].value_counts().count()              #o/p:4911
#trndt1['SBA_Appv'].value_counts().count()            #o/p:6615

#III.Data cleaning on Train Dataset
#Removing null values from columns of Train data
trndt1 = trndt1.dropna(axis=0,subset = ['Name','City','State','Bank','BankState','RevLineCr','DisbursementDate','MIS_Status'])
#trndt1.isnull().sum()    #o/p:only in ChgOffDate column:76035 na values are present now
#After removing na values
#trndt1.shape   #o/p:(104145, 27)
trndt1.head(1)

#Dropping unnecessary columns from Train Dataset
#Dropping 'Unnamed: 0' column(as it is unnecessary for our analysis),and 'ChgOffDate' column as it has more than 75% missing data,from the Dataframe
trndt1.drop(columns = ['Unnamed: 0','ChgOffDate'], axis=1, inplace=True)
trndt1.shape #o/p:(104145, 25)

#Finding Outliers:Ist phase
#In general, outliers are very high and low values.
#Please Note: We are not considering the variables whose standard deviation is less than 85% from its mean, since outliers are expected only in the cases of 
#large variations.
numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']
num_df1 = trndt1.select_dtypes(include=numerics)
## We are not considering the numerical columns that are categorical, in detecting outliers.
categorical_num_columns_df1 = ['Zip','CCSC','ApprovalFY','Term','NoEmp','NewExist','CreateJob','RetainedJob','FranchiseCode','UrbanRural','DisbursementGross',
                               'BalanceGross','ChgOffPrinGr','GrAppv','SBA_Appv','Approval_Date','Approval_Month','Approval_Year','Approval_NewDate',
                               'Disbursement_Date','Disbursement_Month','Disbursement_Year','Disbursement_NewDate']
clmn_app_non_cate_outlier = num_df1.columns[((num_df1.std()/num_df1.mean())*100).abs()>85].tolist()
print("Columns that has outliers in trndt1, \nwhich has a standard deviation of more than 85% from its mean:\n\n ", clmn_app_non_cate_outlier)
#o/p:Columns that has outliers in trndt1,which has a standard deviation of more than 85% from its mean:
#['NoEmp', 'CreateJob', 'RetainedJob', 'FranchiseCode']
#Please Note:In our Train dataset we are not removing any outliers because these outliers can be actual/correct values, not necessary that all outliers 
#are mis-interupted values.
#We will be calculating weight of evidence and information value on the variables present in the dataset going forward,as these are not effected by outliers.

#4.Data type convertions
#Data Type Check for columns
#Convert columns to numeric
#Train Data
#converting MIS_Status(Categorical) column to numeric
#trndt1.MIS_Status.value_counts()
#o/p:P I F    76962
#    CHGOFF   27183
#Name: MIS_Status, dtype: int64
trndt1['MIS_Status'] = trndt1['MIS_Status'].apply(lambda x: 1 if x == 'CHGOFF' else 0)
#trndt1.MIS_Status.value_counts()
#o/p:1 76962
#    0 27183
#Name: MIS_Status, dtype: int64
trndt1.head(1)

#Train Data
trndt1['MIS_Status'].value_counts(normalize=True)
#0    0.738989
#1    0.261011
#Name: MIS_Status, dtype: float64

(trndt1.MIS_Status.value_counts()*100)/len(trndt1)
#o/p:0    73.898891
#    1    26.101109
#Name: MIS_Status, dtype: float64

#Train Data
import matplotlib.pyplot as plt
import seaborn as sns
plt.figure(figsize=(4, 4))
sns.countplot('MIS_Status', data=trndt1, palette='hls')
plt.title('Imbalanced class')
plt.show()

#Checking for Correlation-Phase-1.
#Checking for Correlation for the variables and also w.r.to MIS_Status(Target variable)
#Creating a correlation matrix for Train DataFrame
corrMatrix_trn = trndt1.corr()
print(corrMatrix_trn)

#Using Pearson Correlation
#Visual Representation of the Correlation Matrix of the Train DataFrame using Seaborn
import matplotlib.pyplot as plt
import seaborn as sns
plt.figure(figsize=(12,7))
#cor = df.corr()
sns.heatmap(corrMatrix_trn, annot=True, cmap=plt.cm.Reds, vmin=-0.25,fmt ='.2f',vmax=0.6)
plt.show()

#From Train Data removing the dollar sign and converting the following columns to numeric 
trndt1[['DisbursementGross','BalanceGross','ChgOffPrinGr','GrAppv','SBA_Appv']] = trndt1[['DisbursementGross','BalanceGross','ChgOffPrinGr','GrAppv','SBA_Appv']].replace('[\$,]','',regex=True).astype(float)
#trndt1.head(1)
trndt1[['DisbursementGross','BalanceGross','ChgOffPrinGr','GrAppv','SBA_Appv']] = trndt1[['DisbursementGross','BalanceGross','ChgOffPrinGr','GrAppv','SBA_Appv']].astype(int)
trndt1.head(1)

trndt1['Appr_Amount_Ratio']= trndt1['GrAppv']/trndt1['SBA_Appv']
trndt1['Appr_Amount_Ratio']

trndt1.head(1)

# Adding frequency columns so as to use it while plotting graphs
trndt1["frequency"] = trndt1["GrAppv"] - trndt1["GrAppv"]
len(trndt1["frequency"].astype(int))      #o/p:104145

#5.Derived Metrics
#We will now derive some new columns based on our business understanding that will be helpful in our analysis.
#In Train Data,converting the ApprovalDate column to numeric
trndt1['Approval_Date'],trndt1['Approval_Month'],trndt1['Approval_Year']=trndt1['ApprovalDate'].str.split('-',2).str
#trndt1[['ApprovalDate','Approval_Date','Approval_Month','Approval_Year']].head(1)
#converting the month names to month numbers in Approval Date column
month_numbers = {'Jan': '01', 'Feb': '02', 'Mar': '03', 'Apr': '04', 'May': '05', 'Jun': '06', 'Jul': '07','Aug': '08','Sep': '09','Oct': '10','Nov': '11','Dec': '12'}
for k, v in month_numbers.items(): 
    trndt1['Approval_Month'] = trndt1['Approval_Month'].replace(k, v)
#trndt1['Approval_Month'].head(1)
#concatenating all the 3 columns which are in numeric form as a single New column and checking it's datatype
trndt1['Approval_NewDate'] = trndt1[['Approval_Date','Approval_Month','Approval_Year']].apply(lambda x: ''.join(x),axis=1)
#trndt1[['ApprovalDate','Approval_NewDate']].head(1)
#trndt1['Approval_NewDate'].dtypes        #o/p:dtype('O')
#converting the newly created column from object(string) to int and checking it's datatype
trndt1['Approval_Date'] = trndt1['Approval_Date'].astype(int)
#trndt1['Approval_Date'].dtypes
trndt1['Approval_Month'] = trndt1['Approval_Month'].astype(int)
#trndt1['Approval_Month'].dtypes
trndt1['Approval_Year'] = trndt1['Approval_Year'].astype(int)
#trndt1['Approval_Year'].dtypes
trndt1['Approval_NewDate'] = trndt1['Approval_NewDate'].astype(int)
trndt1['Approval_NewDate'].dtypes          #o/p:dtype('int64')

#In Train Data,converting the DisbursementDate column to numeric
trndt1['Disbursement_Date'],trndt1['Disbursement_Month'],trndt1['Disbursement_Year']=trndt1['DisbursementDate'].str.split('-',2).str
#trndt1[['DisbursementDate','Disbursement_Date','Disbursement_Month','Disbursement_Year']].head(1)
#converting the month names to month numbers in Disbursement Date column
month_numbers = {'Jan': '01', 'Feb': '02', 'Mar': '03', 'Apr': '04', 'May': '05', 'Jun': '06', 'Jul': '07','Aug': '08','Sep': '09','Oct': '10','Nov': '11','Dec': '12'}
for k, v in month_numbers.items(): 
    trndt1['Disbursement_Month'] = trndt1['Disbursement_Month'].replace(k, v)
#trndt1['Disbursement_Month'].head(1)
#concatenating all the 3 columns which are in numeric form as a single New column and checking it's datatype
trndt1['Disbursement_NewDate'] = trndt1[['Disbursement_Date','Disbursement_Month','Disbursement_Year']].apply(lambda x: ''.join(x),axis=1)
#trndt1[['DisbursementDate','Disbursement_NewDate']].head(1)
#o/p:DisbursementDate	Disbursement_Date1
#0	    31-Jul-98	         310798
trndt1['Disbursement_NewDate'].dtypes        #o/p:dtype('O')
#converting the newly created column from object(string) to int and checking it's datatype
trndt1['Disbursement_Date'] = trndt1['Disbursement_Date'].astype(int)
#trndt1['Disbursement_Date'].dtypes 
trndt1['Disbursement_Month'] = trndt1['Disbursement_Month'].astype(int)
#trndt1['Disbursement_Month'].dtypes
trndt1['Disbursement_Year'] = trndt1['Disbursement_Year'].astype(int)
#trndt1['Disbursement_Year'].dtypes  
trndt1['Disbursement_NewDate'] = trndt1['Disbursement_NewDate'].astype(int)
trndt1['Disbursement_NewDate'].dtypes         #o/p:dtype('int64')

#trndt1.shape         #o/p:(104145, 35)
trndt1.head(1)

trndt2 = trndt1.copy()
trndt2.shape          #o/p:(104145, 35)

#trndt1.shape      #o/p:(104145, 35)
trndt1.head(1)

#Finding Outliers:phase-2
#In general, outliers are very high and low values.
#Please Note: We are not considering the variables whose standard deviation is less than 85% from its mean, since outliers are expected only in the cases of 
#large variations.
numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']
num_df1 = trndt1.select_dtypes(include=numerics)
## We are not considering the numerical columns that are categorical, in detecting outliers.
categorical_num_columns_df1 = ['Zip','CCSC','ApprovalFY','Term','NoEmp','NewExist','CreateJob','RetainedJob','FranchiseCode','UrbanRural','DisbursementGross',
                               'BalanceGross','ChgOffPrinGr','GrAppv','SBA_Appv','Approval_Date','Approval_Month','Approval_Year','Approval_NewDate',
                               'Disbursement_Date','Disbursement_Month','Disbursement_Year','Disbursement_NewDate']
clmn_app_non_cate_outlier = num_df1.columns[((num_df1.std()/num_df1.mean())*100).abs()>85].tolist()
print("Columns that has outliers in trndt1, \nwhich has a standard deviation of more than 85% from its mean:\n\n ", clmn_app_non_cate_outlier)
#o/p:Columns that has outliers in trndt1,which has a standard deviation of more than 85% from its mean:
#['NoEmp', 'CreateJob', 'RetainedJob', 'FranchiseCode', 'DisbursementGross', 'BalanceGross', 'MIS_Status', 'ChgOffPrinGr', 'GrAppv', 'SBA_Appv', 
#'Approval_Year', 'Disbursement_Year']
#Please Note:In our Train dataset we are not removing any outliers because these outliers can be actual/correct values, not necessary that all outliers 
#are mis-interupted values.
#We will be calculating weight of evidence and information value on the variables present in the dataset going forward,as these are not effected by ouliers.

#Box Plot
#4.Univariate Analysis
import plotly as py
import plotly.graph_objs as go
import numpy as np
def plot_box_chart(dataframe) :
  
  data = []
  for index, column_name in enumerate(dataframe) :
        data.append(
        go.Box(
            y=dataframe.iloc[:, index],
            name=column_name,
            
         ))   
        
  layout = go.Layout(
    yaxis=dict(
      title="Frequency",
      zeroline=False
    ),
       boxmode='group'
    )
    
  fig = go.Figure(data=data, layout=layout)    
  py.offline.iplot(fig)

#4.Univariate Analysis-Box plot Visualizations
#4a.Continuous variables Visualizations
#GrAppv
plot_box_chart(pd.DataFrame(trndt1["GrAppv"])) # hover cursor above graph
#Analysis:The Graph appears to be highly skewed and seems to have outliers which is pulling down the plotting.Lets check distribution using describe command.
#Most of the Approved Gross amount is distributed between USD 0.99M  to USD 0.2M

trndt1.GrAppv.describe()
#Analysis - Now if we look at the mean and max value there is huge difference and needs to be corrected. Clearly max value is an outlier which needs to be 
#corrected otherwise we will have huge deviation in our analysis.

outlier_value_GrAppv = trndt1["GrAppv"].quantile(0.995)
trndt1 = trndt1[trndt1["GrAppv"] < outlier_value_GrAppv]
trndt1["GrAppv"].describe()

plot_box_chart(pd.DataFrame(trndt1["GrAppv"]))
#Analysis-A better plotting when compared with the previous one.Still we can observe that they there are outliers present in the data.
#Most of the Approved Gross amount is distributed between USD 0.01M  to USD 0.1M.

#Term
plot_box_chart(pd.DataFrame(trndt1["Term"])) # hover cursor above graph
#Analysis - The Graph appears to be highly skewed and seems to have outliers in the data.
#Most of the Term Period is distributed between 60 to 90 Months.

#DisbursementGross
plot_box_chart(pd.DataFrame(trndt1["DisbursementGross"])) # hover cursor above graph
#Analysis:The Graph appears to be highly skewed and seems to have outliers which is pulling down entire plotting.Lets check distribution using describe command.
#Most of the Gross Disbursement amount is distributed between USD 0.01M  to USD 0.1M

trndt1.DisbursementGross.describe()
#Analysis - Now if we look at the mean and max value there is huge difference and needs to be corrected. Clearly max value is an outlier which needs to be 
#corrected otherwise we will have huge deviation in our analysis.

outlier_value_DisGr = trndt1["DisbursementGross"].quantile(0.995)
trndt1 = trndt1[trndt1["DisbursementGross"] < outlier_value_DisGr]
trndt1["DisbursementGross"].describe()

plot_box_chart(pd.DataFrame(trndt1["DisbursementGross"]))
#Analysis-Comparatively a better plotting than the previous one.Still we can observe that they there are outliers present in the data.
#Most of the Gross Disbursement amount is distributed between USD 0.01M  to USD 0.122M.

#SBAAppv
plot_box_chart(pd.DataFrame(trndt1["SBA_Appv"])) # hover cursor above graph
#Analysis:The Graph appears to be highly skewed and seems to have outliers which is pulling down the plotting.Lets check distribution using describe command.
#Most of the Approved SBA amount is distributed between USD 0.01M  to USD 0.88M.

trndt1.SBA_Appv.describe()
#Analysis - Now if we look at the mean and max value there is huge difference and needs to be corrected. Clearly max value is an outlier which needs to be 
#corrected otherwise we will have huge deviation in our analysis.

outlier_value_SbaAppv = trndt1["SBA_Appv"].quantile(0.995)
trndt1 = trndt1[trndt1["SBA_Appv"] < outlier_value_SbaAppv]
trndt1["SBA_Appv"].describe()

#SBAAppv
plot_box_chart(pd.DataFrame(trndt1["SBA_Appv"])) # hover cursor above graph
#Analysis-Comparatively a better plotting when compared to the previous one.Still we can observe that they there are outliers present in the data.
#Most of the SBA Approved amount is distributed between USD 1k to USD 90k.

trndt1.head(1)

#Bar Chart
#4.Univariate Analysis
import plotly as py
import plotly.graph_objs as go
def plot_bar_chart(plotting_frame,x_col_name,y_col_name) :
            
        x_axis_title = x_col_name.title()
        y_axis_title = y_col_name.title()
        
        graph_title = "Bar Chart [" + x_axis_title.title() + " Vs " + y_axis_title.title() + "]"
        
        layout = go.Layout(
             title = graph_title,
             yaxis=dict(
                title=y_axis_title
             ),
             xaxis=dict(
                 title=x_axis_title
             )
        )

        data_to_be_plotted = [
            go.Bar(
                x=plotting_frame[x_col_name], 
                y=plotting_frame[y_col_name],marker_color='lightsalmon'
            )
        ]


        figure = go.Figure(data=data_to_be_plotted,layout=layout)
        py.offline.iplot(figure)

#4.Univariate Analysis Bar Chart Visualizations
#4b.Categorical Variables Visualizations
#Plotting for Name
plot_NM = trndt1[3030:3040].groupby("Name").frequency.count().reset_index()
plot_bar_chart(plot_NM,"Name","frequency")
#Analysis:Almost all the clients(Name) have applied for loans.

#Plotting for city
plot_CY = trndt1[180:190].groupby("City").frequency.count().reset_index()
plot_bar_chart(plot_CY,"City","frequency")
#Analysis:Almost all the cities has equal number of applicants applying for loans.

#Plotting for State
plot_ST = trndt1.groupby("State").frequency.count().reset_index()
plot_bar_chart(plot_ST,"State","frequency")
#Analysis:CA(California) State has maximum number of applicants applying for loans.

#Plotting for Bank
plot_BK = trndt1[:10].groupby("Bank").frequency.count().reset_index()
plot_bar_chart(plot_BK,"Bank","frequency")
#Analysis:BANK OF AMERICA NATL ASSOC has maximum number of applicants applying for loans.

#Plotting for BankState
plot_BKST = trndt1.groupby("BankState").frequency.count().reset_index()
plot_bar_chart(plot_BKST,"BankState","frequency")
#Analysis:The Bank located at NC(North Carolina) State has maximum number of applicants applying for loans.

#Plotting for UrbanRural
plot_UR = trndt1.groupby("UrbanRural").frequency.count().reset_index()
plot_bar_chart(plot_UR,"UrbanRural","frequency")
#Analysis:There are maximum number of applicants from Urban Areas who applied for loans.
#Undefined applicants(2) loan percenatge is significant which can result in charge-off loans, bank should work towards lowering this number of Undefined applicants of loan.

#plotting for LowDoc
plot_LD = trndt1.groupby("LowDoc").frequency.count().reset_index()
plot_bar_chart(plot_LD,"LowDoc","frequency")
#Analysis:Applicants applying for loans are high for those who need to submit more than one page Documentation.

#Plotting for Approval Fiscal Year
plot_APPFY = trndt1[:50].groupby("ApprovalFY").frequency.count().reset_index()
plot_bar_chart(plot_APPFY,"ApprovalFY","frequency")
#Analysis - Applicants have taken maximum number of loans in Year 2006.

#Pie Chart
def plot_pie_chart(plotting_frame,x_col_name,y_col_name) : 
        
        labels = plotting_frame[x_col_name].tolist()
        values = plotting_frame[y_col_name].tolist()

        trace = go.Pie(labels=labels, values=values)

        py.offline.iplot([trace])

#Pie Chart Visualization
#Plotting MIS_Status
import ipywidgets as widgets
plot_MIS_STATUS = trndt1.groupby("MIS_Status").frequency.count().reset_index()
plot_pie_chart(plot_MIS_STATUS,"MIS_Status","frequency")
#Analysis - Looking at above distribution company is in pretty good shape as applicants are finishing off the loan ammount in timely basis as it is 
#whooping 73.6% but still needs to see the distribution across different categories and against loan amount taken. We still see charge off percentage as 26.4% from total distribution

#Group Bar Chart
import math
def plot_group_bar_chart(plot,col,hue) : 
    hue_col = pd.Series(data = hue)
    fig, ax = plt.subplots()
    width = len(plot[col].unique()) + 3 + 2*len(hue_col.unique())
    fig.set_size_inches(width , 5)
    ax = sns.countplot(data = loan_plot, x= col, order=plot[col].value_counts().index,hue = hue,palette="Set2") 
    
    for p in ax.patches:
      # Some segment wise value we are getting as Nan as respective value not present to tackle the Nan using temp_height
      temp_height = p.get_height()
      if math.isnan(temp_height):
        temp_height = 0.01

      ax.annotate("{:1.1f}%".format((temp_height*100)/float(len(loan_plot))), (p.get_x()+0.02, (temp_height))) 
    
    plt.show()

def validate_year(date) :
    temp = date.split('-')[1]
    lenght = len(temp)
    if lenght == 2 :
        temp = "20"+temp
    else :
        temp = "200"+temp
        
    return temp

#Group Bar Chart Visualizations
#Now showing distribution across categories
#How the distribution looks like for Loan Term vs Mis_status?
#import math
loan_plot = trndt1[:12][["Term","MIS_Status"]]
plot_group_bar_chart(loan_plot,"Term","MIS_Status")
#Analysis:Approx 34% of applicants applied loan for 84 months term period.

#How the distribution looks like for Loan Term vs Mis_status?
loan_plot = trndt1[:25][["State","MIS_Status"]]
plot_group_bar_chart(loan_plot,"State","MIS_Status")
#Analysis:About 20% Applicants who applied for loan are from  CA(California) State.

#How the distribution looks like for Urban Rural vs Mis_status?
loan_plot = trndt1[["UrbanRural","MIS_Status"]]
plot_group_bar_chart(loan_plot,"UrbanRural","MIS_Status")
#Analysis:Approximately 55% of applicants applied for loan are from Urban Areas.

#How the distribution looks like for LowDoc vs Mis_status?
loan_plot = trndt1[:10][["LowDoc","MIS_Status"]]
plot_group_bar_chart(loan_plot,"LowDoc","MIS_Status")
#Analysis:About 90% of loan applicants applied for loan are those,where the banks asked them to submit more than a single page document for thorough analysis for the 
#inorder for the loan to be approved.

#5.Bivariate/Multivariate Analysis
#Finding correlation between amount fields:
trndt1.columns

## picking relevent (continuous variable) fields to check if they are correlated 
col_for_corr = ['CreateJob','RetainedJob','DisbursementGross','BalanceGross','GrAppv','SBA_Appv','Appr_Amount_Ratio','ApprovalFy','ChgOffPrinGr']

loan_corr = trndt1.filter(col_for_corr)
#loan_corr

corr_matrix_trn = loan_corr.corr(method ='pearson')
corr_matrix_trn

##Visual Representation of the Correlation Matrix for a few selected continuous variables using Seaborn
import matplotlib.pyplot as plt
import seaborn as sns
plt.subplots(figsize=(12,7))
sns.heatmap(corr_matrix_trn, 
            xticklabels=corr_matrix_trn.columns.values,
            yticklabels=corr_matrix_trn.columns.values,annot= True,fmt='.2f',cmap=plt.cm.Reds)
plt.show()
#Analysis - It is clear from the Heatmap that how GrAppv and SBA_Appv are closely interrelated.So these two variables can be considered for further analysis.

#Multivariate Box Plot Visualization
#What is the State and the amount corresponding to each Mis_status
plt.figure(figsize=(20,16))
sns.boxplot(data = trndt1[:5000], x='GrAppv', y='State', hue ='MIS_Status')
plt.show()
#Analysis-By looking at plot we can see that CA(California) and TX(Texas) have a lot of outliers in charge off and which will contribute to more losses for the bank.
#Small Business Association also needs to look at them closely and hence while giving loans for above two categories bank should have more proactive checks.

#Bivariate Bar Chart Visualization
#Lets see how defaulting related to NewExist
loan_plot = trndt1[["NewExist","MIS_Status"]]
plot_group_bar_chart(loan_plot,"NewExist","MIS_Status")
#Analysis - Loan taken in maximum number by Existing Businesses.Approx 65% Loan has been taken.

#6.Defaulters Probability Analysis
#Graphs and Insights
trndt1.head(1)
#Probability of Default Loan = (Occurance of Default Loan) / (Total Loan disbursed)
#Function to calculate defaulter percentages
def calculate_defaulter_percentage(dataframe,column) :
    def_tab = pd.crosstab(dataframe[column], dataframe['MIS_Status'],margins=True)
    def_tab['All'] = def_tab[0] +  def_tab[1]
    def_tab['Loan Default Probability'] = round((def_tab[1]/def_tab[0]),3)
    def_tab = def_tab[0:-1] # removing last row with sum totol 
    return def_tab

#Bar Line Chart
#Function to plot graph between bivariate and derived variables
def plot_bar_line_chart(dataframe,column,stacked=False):
    
    plot = calculate_defaulter_percentage(dataframe,column)
    
    display(plot)
    
    #initializing line plot
    linePlot = plot[['Loan Default Probability']] 
    line = linePlot.plot(figsize=(20,8), marker='o',color = 'r',lw=2)
    line.set_title(dataframe[column].name.title()+' vs Loan Default Probability',fontsize=20,weight="bold")
    line.set_xlabel(dataframe[column].name.title(),fontsize=14)
    line.set_ylabel('Loan Default Probability',color = 'r',fontsize=20)
    
    #initializing bar plot
    barPlot =  plot.iloc[:,0:3] 
    bar = barPlot.plot(kind='bar',ax = line,rot=1,secondary_y=True,stacked=stacked)
    bar.set_ylabel('Number of Applicants',color = 'r',fontsize=20)
    
    plt.show()

#calculate_defaulter_percentage(dataframe=trndt1,column='NewExist')

#Bar Line Chart Visualizations
##Plotting NewExist probablility with defaulting behaviour
plot_bar_line_chart(trndt1,"NewExist")
#Analysis - Applicants who are from New Existing Business are more probable of Likely to be Defaulter.

##Plotting RetainedJob probablility with defaulting behaviour
plot_bar_line_chart(trndt1[:100],"RetainedJob")
#Analysis - Applicants whose are Retained in jobs for 3 yrs are more probable of Likely to be Defaulter.

#Plotting RevLineCr probablility with defaulting behaviour
plot_bar_line_chart(trndt1[:25],"RevLineCr")
##Analysis - Applicants who are asking for Revolving credit with category 'N' are more probable of Likely to be Defaulter.

#Plotting LowDoc probablility with defaulting behaviour
plot_bar_line_chart(trndt1,"LowDoc")
##Analysis - Applicants who are asked to submit more than one page document are more probable of Likely to be Defaulter.

##Plotting Approval probablility with defaulting behaviour
plot_bar_line_chart(trndt1,"Approval_Year")
#Analysis - For the Applicants whose loans are Approved in year 2006 are more probable of Likely to be Defaulter.

##Plotting Disbursement probablility with defaulting behaviour
plot_bar_line_chart(trndt1,"Disbursement_Year")
#Analysis - Applicants who will be receiving loans in year 2006 are more probable of Likely to be Defaulter.

#Plotting Location / State probablility with defaulting behaviour
#Removing  Address state those having vary small value, which will impact the Probability Analysis
filter_states = trndt1.BankState.value_counts()
filter_states = filter_states[(filter_states < 10)]
loan_filter_states = trndt1.drop(labels = trndt1[trndt1.BankState.isin(filter_states.index)].index)
plot_bar_line_chart(loan_filter_states,"BankState",True)
#Analysis - There are multiple States/Provinces with high probability of Likely to be a Defaulter ,highest being 'VA(Virginia)' at 74%

# Create Bins for range of SBA Approval Amount
#bins = [0, 5000, 10000, 15000, 20000, 25000,40000]
#slot = ['0-5000', '5000-10000', '10000-15000', '15000-20000', '20000-25000','25000 and above']
#bins1 = [0, 25000, 50000, 75000, 100000,1000000]
#slots1 = ['0-25000', '25000-50000', '50000-75000', '75000-100000', '100000 and above']
#trndt1['SBA_Appv_Amnt_Range'] = pd.cut(trndt1['SBA_Appv'], bins1, labels=slots1)

# Create Bins for range of Gross Approval Amount
#bins2 = [0, 25000, 50000, 75000, 100000,1000000]
#slots2 = ['0-25000', '25000-50000', '50000-75000', '75000-100000', '100000 and above']
#trndt1['GrAppv_Amnt_Range'] = pd.cut(trndt1['GrAppv'], bins2, labels=slots2)

# Create Bins for Range of Term period
#bins3 = [0, 50,100, 150, 200, 250, 300, 350, 400, 450 , 500]
#slots3 = ['0-50', '50-100', '100-150', '150-200', '200-250','250-300', '300-350','350-400','400 and above']
bins3 = [0,100,200,300,400,500]
slots3 = ['0-100','100-200','200-300','300-400','400 and above']
trndt1['Term_period_Range'] = pd.cut(trndt1['Term'], bins3, labels=slots3)

trndt1.head(2)

#Gross Approval Amount Range vs Probability Likely to be Defaulter
plot_bar_line_chart(trndt1,"GrAppv_Amnt_Range",stacked=True)
#Analysis - As the Gross Approval Amount is increasing the probability that person will default is increasing with highest of 48% at (25000-50000)Approval Amount bracket.

#Term Period Range vs Probability Likely to be Defaulter
plot_bar_line_chart(trndt1,"Term_period_Range",stacked=True)
Analysis - As the Term Period is increasing the probability that person will default is increasing with highest at 400 & above bracket.

#SBA Approval Amount Range vs Probability Likely to be Defaulter
plot_bar_line_chart(trndt1,"SBA_Appv_Amnt_Range",stacked=True)
#Analysis - As the SBA Approval Amount is increasing the probability that person will default is increasing with highest of 49% at (0-25000)Approval Amount bracket.

#7.Conclusion
Target Variable
MIS Status
Top-10 Major variables to consider for loan defaulter prediction:
BankState
RevLineCr
LowDoc
GrAppv
SBA Appv
UrbanRural
ApprovalFY
Retained Job
Approval_Year
Disbursement Year

#As a part of EDA we calculate Weight of Evidence and Information Value
#WOE:The weight of evidence tells the predictive power of an independent variable in relation to the dependent variable.
#Since it evolved from credit scoring world, it is generally described as a measure of the separation of good and bad customers. 
#"Bad Customers" refers to the customers who defaulted on a loan. and "Good Customers" refers to the customers who paid back loan.
#Information Value (IV):Information value is one of the most useful technique to select important variables in a predictive model. 
#It helps to rank variables on the basis of their importance.

#After removing null values and removing a column('chgoffdate') as it has more than 75% missing data
#After converting $ columns,ApprovalDate and DisbursementDate columns to numeric 
##checking for woe and iv 
import numpy as np
def iv18_woe18(data, target, bins=10, show_woe=False):
    
    #Empty Dataframe
    newDF64,newDF65,newDF66,newDF67,woeivDF18 = pd.DataFrame(), pd.DataFrame(), pd.DataFrame(), pd.DataFrame(), pd.DataFrame()
    
    #Extract Column Names
    cols = data.columns
    
    #Run WOE and IV on all the independent variables
    for ivars in cols[~cols.isin([target])]:
        if (data[ivars].dtype.kind in 'bifc') and (len(np.unique(data[ivars]))>10):
            binned_x = pd.qcut(data[ivars], bins,  duplicates='drop')
            d0 = pd.DataFrame({'x': binned_x, 'y': data[target]})
        else:
            d0 = pd.DataFrame({'x': data[ivars], 'y': data[target]})
        d = d0.groupby("x", as_index=False).agg({"y": ["count", "sum"]})
        d.columns = ['Cutoff', 'N', 'Events']
        d['% of Events'] = np.maximum(d['Events'], 0.5) / d['Events'].sum()
        d['Non-Events'] = d['N'] - d['Events']
        d['% of Non-Events'] = np.maximum(d['Non-Events'], 0.5) / d['Non-Events'].sum()
        d['WoE_18'] = np.log(d['% of Events']/d['% of Non-Events'])
        d['IV_18'] = d['WoE_18'] * (d['% of Events'] - d['% of Non-Events'])
        d.insert(loc=0, column='Variable', value=ivars)
        print("Information value of " + ivars + " is " + str(round(d['IV_18'].sum(),6)))
        print("Weight Of Evidence of " + ivars + " is " + str(round(d['WoE_18'].sum(),6)))
        temp_9 =pd.DataFrame({"Variable" : [ivars], "IV_18" : [d['IV_18'].sum()]}, columns = ["Variable", "IV_18"])
        tempWOE_18 =pd.DataFrame({"variable" : [ivars], "WoE_18" : [d['WoE_18'].sum()]}, columns = ["variable", "WoE_18"])
        tempIV_18 =pd.DataFrame({"variable" : [ivars], "IV_18" : [d['IV_18'].sum()]}, columns = ["variable", "IV_18"])        
        tempIVWOE_18 =pd.DataFrame({"variable" : [ivars], "IV_18" : [d['IV_18'].sum()], "WoE_18" : [d['WoE_18'].sum()]}, columns = ["variable", "IV_18", "WoE_18"])

        newDF64=pd.concat([newDF64,temp_9], axis=0)
        newDF65=pd.concat([newDF65,tempWOE_18], axis=0)
        newDF66=pd.concat([newDF66,tempIV_18], axis=0)
        newDF67=pd.concat([newDF67,tempIVWOE_18], axis=0)
        woeivDF18=pd.concat([woeivDF18,d], axis=0)
        ##newDF3=pd.concat([newDF3,tempWOE], axis=0)
        #woeivDF1=pd.concat([woeivDF1,d], axis=0)

        #Show WOE Table
        if show_woe == True:
            print(d)
    return newDF64,newDF65,newDF66,newDF67,woeivDF18

final_18,woe_18,iv_18,IvWoe_18,woeiv_18 = iv18_woe18(data = trndt1, target = 'MIS_Status', bins=10, show_woe = True)
final_18
#print(woe_18)
#print(iv_18)
#print(IvWoe_18)
#print(woeiv_18)

#Weight of Evidence(WOE):
#11-columns with +ve WOE:Name,City,DisbursementDate,ApprovalDate,ChgOffPrinGr,RetainedJob,RevLineCr,CCSC,BalanceGross,CreateJob,Disbursement_Month.
#21-columns with _ve WOE:Approval_NewDate,Approval_Date,Approval_Month,Zip,Disbursement_NewDate,Disbursement_Date,DisbursementGross,NoEmp,FranschiseCode,
#UrbanRural,ApprovalFY,Term,GrAppv,SBA_Appv,Approval_Year,NewExist,LowDoc,Disbursement_Year,State,BankState,Bank.
woe_18.sort_values(by='WoE_18',ascending=False)

#INFORMATION VALUE(IV):
##Rules related to Information Value
#Information Value	          Variable Predictiveness                columns
#Less than 0.02	              Not useful for prediction              BalanceGross,Approval_NewDate,Approval_Date,Disbursement_Month,NewExist,FranchiseCode,Approval_Month,Disbursement_Date(8)
#0.02 to 0.1	                Weak predictive Power                  CreateJob,Disbursement_NewDate,Zip,NoEmp,State,DisbursementGross,CCSC(7)
#0.1 to 0.3	                  Medium predictive Power                LowDoc,GrAppv,RetainedJob,SBA_Appv,RevLineCr(5)
#0.3 to 0.5	                  Strong predictive Power                BankState,UrbanRural,Approval_Year,ApprovalFY,Disbursement_Year(5)
#>0.5	                        Suspicious Predictive Power            City,Bank,ApprovalDate,DisbursementDate,Name,Term,ChgOffPrinGr(7)

##predictors
#not considering:notuseful(8)+weak(7) = Total = 15
#considering:medium(5)+strong(5) = Total = 10
#if we consider:suspicious(7) = Total = 10+7 = 17
#or else(medium+strong=5+5=10)
iv_18.sort_values(by='IV_18',ascending=False)

IvWoe_18.sort_values(by='IV_18',ascending=False)

woeiv_18  #o/p:120074 rows × 9 columns

#plotting Weight Of Evidence
import plotly
import plotly.graph_objs as go
from  plotly.offline import iplot,plot
data = [go.Bar(
            x=IvWoe_18['variable'],
            y=IvWoe_18['WoE_18'],
            text=IvWoe_18['variable'],
            marker=dict(
            color='rgb(158,20,25)',
            line=dict(
            color='rgb(8,48,107)',
            width=1.5,
        )
    ),
    opacity=0.6
    )]


layout = go.Layout(
    title='Weight of Evidence(WOE)',
        xaxis=dict(
        title='Features',
            tickangle=-45,
        tickfont=dict(
            size=10,
            color='rgb(107, 107, 107)'
        )
    ),
    yaxis=dict(
        title='Weight of Evidence(WOE)',
        titlefont=dict(
            size=14,
            color='rgb(107, 107, 107)'
        ),
        tickfont=dict(
            size=14,
            color='rgb(107, 107, 107)'
        )
    ),
)

plotly.offline.iplot({
    "data": data,'layout':layout
})
#From the plot below we observe that the variable 'Name' has highest predictive power when compared to other variables which are City,DisbursementDate,
#ApprovalDate and Bank.And the weight of Evidence plotted here is observed for pure multi-labelled categorical Data.

#Plotting Information Values
data = [go.Bar(
            x=IvWoe_18['variable'],
            y=IvWoe_18['IV_18'],
            text=IvWoe_18['variable'],
            marker=dict(
            color='rgb(58,256,225)',
            line=dict(
            color='rgb(8,48,107)',
            width=1.5,
        )
    ),
    opacity=0.6
    )]


layout = go.Layout(
    title='Information Values',
        xaxis=dict(
        tickangle=-45,
        title='Features',
        tickfont=dict(
            size=10,
            color='rgb(7, 7, 7)'
        )
    ),
    yaxis=dict(
        title='Information Value(IV)',
        titlefont=dict(
            size=14,
            color='rgb(107, 107, 107)'
        ),
        tickfont=dict(
            size=14,
            color='rgb(107, 107, 107)'
        )
    ),
)

plotly.offline.iplot({
    "data": data,'layout':layout
})

#Conclusion:From the plot we observe that the following features are found to be good predictors and have a strong relationship to the Goods/Bads odds ratio.
##Based on Information Value Interpretation we will be choosing the following columns for model building:
#Information Value            Variable Predictiveness                Columns
#0.1 to 0.3	                  Medium predictive Power                LowDoc,GrAppv,RetainedJob,SBA_Appv,RevLineCr(5)
#0.3 to 0.5	                  Strong predictive Power                BankState,UrbanRural,Approval_Year,ApprovalFY,Disbursement_Year(5)